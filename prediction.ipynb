{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from S3: my-feature-store-data/models/best_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GradientBoostingRegressor from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DummyRegressor from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load model: <class 'numpy.random._mt19937.MT19937'> is not a known BitGenerator module.. Training new model.\n",
      "\n",
      "Model Performance:\n",
      "  Target: aqi_index\n",
      "    RMSE: 0.2403\n",
      "    MAE: 0.1191\n",
      "    R²: 0.9314\n",
      "  Target: Calculated_AQI\n",
      "    RMSE: 7.1535\n",
      "    MAE: 1.5161\n",
      "    R²: 0.9952\n",
      "\n",
      "Predicted values for the next 3 days:\n",
      "         Date  Predicted_AQI  Predicted_Calculated_AQI\n",
      "0  2025-04-30           3.64                    232.20\n",
      "1  2025-05-01           3.74                    233.26\n",
      "2  2025-05-02           3.74                    234.68\n",
      "\n",
      "Top 10 most important features:\n",
      "           Feature  Importance\n",
      "7             pm10    0.444113\n",
      "1               co    0.379451\n",
      "0            index    0.132354\n",
      "6            pm2_5    0.018193\n",
      "5              so2    0.009151\n",
      "4               o3    0.006023\n",
      "3              no2    0.004908\n",
      "9   temperature_2m    0.001786\n",
      "8              nh3    0.001293\n",
      "12  wind_speed_10m    0.000457\n",
      "\n",
      "Retrained model saved to S3.\n",
      "Scaler saved to S3.\n",
      "Prediction results saved locally as 'prediction_results.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Step 1: Load Data from AWS S3 ---\n",
    "bucket_name = 'my-feature-store-data'\n",
    "data_key = 'pipeline-data/data.csv'\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=data_key)\n",
    "df = pd.read_csv(obj['Body'])\n",
    "\n",
    "# Process date columns\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.dropna(subset=['aqi_index', 'Calculated_AQI'])  # Remove rows with missing target values\n",
    "\n",
    "# Define features and target\n",
    "target_columns = ['aqi_index', 'Calculated_AQI']\n",
    "date_columns = ['year', 'month', 'day', 'hour']\n",
    "features = [col for col in df.columns if col not in target_columns and col != 'date']\n",
    "\n",
    "X = df[features]\n",
    "y = df[target_columns]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# initialize Y_pred to None\n",
    "Y_predicted = None\n",
    "# --- Step 2: Load or Train Model ---\n",
    "try:\n",
    "    model_key = 'models/best_model.pkl'\n",
    "    print(f\"Attempting to load model from S3: {bucket_name}/{model_key}\")\n",
    "    \n",
    "    # Check if model exists\n",
    "    s3.get_object(Bucket=bucket_name, Key=model_key)\n",
    "    print(f\"Model file exists at {bucket_name}/{model_key}\")\n",
    "    # Download model\n",
    "    model_buffer = BytesIO()\n",
    "    with open(model_buffer, 'wb') as f:\n",
    "        s3.download_fileobj(bucket_name, model_key, f)\n",
    "    model_buffer.seek(0)\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = joblib.load(model_buffer)\n",
    "    print(\"Successfully loaded pre-trained model from S3\") \n",
    "    y_pred = model.predict(X_test_scaled) \n",
    "    Y_predicted = y_pred\n",
    "except Exception as e:\n",
    "    print(f\"Could not load model: {str(e)}. Training new model.\")\n",
    "    # Train new model\n",
    "    model = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    Y_predicted = y_pred\n",
    "\n",
    "# --- Step 3: Evaluate Model ---\n",
    "\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "for i, col in enumerate(target_columns):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test.iloc[:, i], Y_predicted[:, i]))\n",
    "    mae = mean_absolute_error(y_test.iloc[:, i], Y_predicted[:, i])\n",
    "    r2 = r2_score(y_test.iloc[:, i], Y_predicted[:, i])\n",
    "    \n",
    "    print(f\"  Target: {col}\")\n",
    "    print(f\"    RMSE: {rmse:.4f}\")\n",
    "    print(f\"    MAE: {mae:.4f}\")\n",
    "    print(f\"    R²: {r2:.4f}\")\n",
    "\n",
    "# --- Step 4: Predict AQI for Next 3 Days ---\n",
    "start_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)\n",
    "future_dates = [start_date + timedelta(days=i) for i in range(3)]\n",
    "\n",
    "# Prepare future features\n",
    "future_features = pd.DataFrame({\n",
    "    \"year\": [d.year for d in future_dates],\n",
    "    \"month\": [d.month for d in future_dates],\n",
    "    \"day\": [d.day for d in future_dates],\n",
    "    \"hour\": [12] * 3,  # Predict for noon\n",
    "})\n",
    "\n",
    "# Find most recent data for April 2025\n",
    "recent_data = df[(df['year'] == 2025) & (df['month'] == 4)]\n",
    "\n",
    "numeric_features = [col for col in features if col not in date_columns]\n",
    "\n",
    "if len(recent_data) > 0:\n",
    "    recent_averages = recent_data[numeric_features].mean().to_dict()\n",
    "else:\n",
    "    similar_season_data = df[df['month'] == 4]\n",
    "    if len(similar_season_data) > 0:\n",
    "        recent_averages = similar_season_data[numeric_features].mean().to_dict()\n",
    "    else:\n",
    "        recent_averages = df[numeric_features].mean().to_dict()\n",
    "\n",
    "for feature, value in recent_averages.items():\n",
    "    future_features[feature] = value\n",
    "\n",
    "# Add small variations to weather-related features\n",
    "for i in range(1, len(future_features)):\n",
    "    for feature in ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'wind_speed_10m', 'wind_direction_10m', 'surface_pressure']:\n",
    "        if feature in future_features.columns:\n",
    "            variation = np.random.uniform(-0.05, 0.05)\n",
    "            future_features.loc[i, feature] = future_features.loc[i-1, feature] * (1 + variation)\n",
    "\n",
    "# Fill missing columns\n",
    "required_columns = X_train.columns.tolist()\n",
    "for col in required_columns:\n",
    "    if col not in future_features.columns:\n",
    "        future_features[col] = X_train[col].mean()\n",
    "\n",
    "# Order columns correctly\n",
    "future_features = future_features[required_columns]\n",
    "\n",
    "# Scale and predict\n",
    "future_scaled = scaler.transform(future_features)\n",
    "predictions = model.predict(future_scaled)\n",
    "\n",
    "prediction_results = pd.DataFrame({\n",
    "    \"Date\": [d.strftime(\"%Y-%m-%d\") for d in future_dates],\n",
    "    \"Predicted_AQI\": np.round(predictions[:, 0], 2),\n",
    "    \"Predicted_Calculated_AQI\": np.round(predictions[:, 1], 2)\n",
    "})\n",
    "\n",
    "print(\"\\nPredicted values for the next 3 days:\")\n",
    "print(prediction_results)\n",
    "\n",
    "# --- Step 5: Feature Importance ---\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': model.feature_importances_,\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "elif hasattr(model, 'estimators_'):\n",
    "    estimator = model.estimators_[0]\n",
    "    if hasattr(estimator, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': estimator.feature_importances_,\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 most important features (from first target model):\")\n",
    "        print(feature_importance.head(10))\n",
    "\n",
    "# --- Step 6: Save Model and Scaler to S3 ---\n",
    "# Save model\n",
    "model_buffer = BytesIO()\n",
    "joblib.dump(model, model_buffer)\n",
    "model_buffer.seek(0)\n",
    "s3.upload_fileobj(model_buffer, Bucket=bucket_name, Key='models/retrained_multioutput_model.pkl')\n",
    "print(\"\\nRetrained model saved to S3.\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_buffer = BytesIO()\n",
    "joblib.dump(scaler, scaler_buffer)\n",
    "scaler_buffer.seek(0)\n",
    "s3.upload_fileobj(scaler_buffer, Bucket=bucket_name, Key='models/scaler.pkl')\n",
    "print(\"Scaler saved to S3.\")\n",
    "\n",
    "# Save predictions locally\n",
    "prediction_results.to_pickle(\"prediction_results.pkl\")\n",
    "print(\"Prediction results saved locally as 'prediction_results.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773da10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save `prediction_results` to a file\n",
    "prediction_results.to_pickle(\"prediction_results.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f22f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
